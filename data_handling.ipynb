{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f1b9335-34a4-4839-baea-9c5c9dc4546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b84ee-459c-4f05-ae76-44b534c0fc6a",
   "metadata": {},
   "source": [
    "# Transforming the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93680dea-c089-4a59-8fa2-dd7f7a12c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/MTA_Subway_Hourly_Ridership_Jan_Through_May_2025.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c6501-54bc-4365-8f81-3dd62046cca1",
   "metadata": {},
   "source": [
    "This dataframe includes the number of riders, per station complex, per hour, for every hour in every day for the months January-April.\n",
    "\n",
    "My goal is to see if I can create a model to forecast subway ridership. It should work like this: Given some ridership, that is the riders for every hour of a given time period, can I forecast the ridership in the near future? For a preliminary investigation, I will consider a lookback length of 24 hours, and a prediction horizon of 6 hours. \n",
    "\n",
    "Naturally, I need the ridership by hour for each 24 hour period, and the ridership of the next 6 hours, for each station, to construct my training dataset. The dataset as is needs to be transformed\n",
    "\n",
    "Exogenous features to consider alongside time series:\n",
    "\n",
    "- It may be important to account for the day of the week. For example, it will be more difficult to predict Saturday from Friday, than say Tuesday from Monday, due to the distinct nature of weekend and weekday ridership. \n",
    "\n",
    "- Furthermore, station information may need to be taken into consideration. Different stations experience different daily ridership patterns. Popular stations such as Times Square-42nd St. have a lot of ridership constantly, while some stations on the outer edges of the system may have little to no ridership some days.\n",
    "\n",
    "    - Note! One way to go about this is to include the (normazlied) lat/long coordinates with datapoints. This is better than simply one-hot encoding by station_complex because lat/long coordinates will contain information of where stations are with respect to one another. In a previous project, I identified a strong correlation between daily ridership pattern (on weekends) and location.\n",
    "      \n",
    "    - Note! In this dataset, there are sometimes multiple lat/long pairs listed for a given station_complex. This phenomenon is demonstrated in lat_long_discrepancy/lat_long_discrepancy.ipynb . Because I believe this is an error, I will take the mean of the lats/longs per station_complex as that station's true lat/long for my purposes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a45ba4-c3ba-4b4b-b979-7038df650453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a df which contains station_complex, station_complex_id, latitude, longitude\n",
    "# so I can keep just station_complex_id for now, and later merge station_info_df with on=station_complex_id to \n",
    "# recover latitudes and longitudes.\n",
    "\n",
    "# As mentioned above, I take the mean(latitude) and mean(longitude) per station_complex.\n",
    "\n",
    "station_info_df = df[df[\"transit_mode\"] == \"subway\"][['station_complex_id', 'latitude', 'longitude']].drop_duplicates().reset_index(drop= True)\n",
    "station_info_df = station_info_df.groupby([\"station_complex_id\"], as_index = False).agg({\"latitude\": 'mean', \"longitude\": 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baec8fac-3aea-463e-9a32-d43981b999b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the latitudes and longitudes to zero mean and unit variance.\n",
    "\n",
    "station_info_df['latitude'] = (station_info_df['latitude'] - station_info_df['latitude'].mean())/station_info_df['latitude'].std()\n",
    "station_info_df['longitude'] = (station_info_df['longitude'] - station_info_df['longitude'].mean())/station_info_df['longitude'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f16da1cf-d480-4182-83d9-73d92a82c49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming df to a workable form\n",
    "\n",
    "# Only considering Subway!\n",
    "df = df[df[\"transit_mode\"] == \"subway\"]\n",
    "\n",
    "# Dropping Other Unneccesary Rows!\n",
    "df = df.drop([\"transfers\", \"Georeference\", \"station_complex\", \"latitude\", \"longitude\", \"borough\"], axis = 1)\n",
    "\n",
    "# Pandas datetime format.\n",
    "df['transit_timestamp'] = pd.to_datetime(df['transit_timestamp'], format = \"%m/%d/%Y %I:%M:%S %p\")\n",
    "\n",
    "# Keeping only date and hour, then discarding transit_timestamp\n",
    "df['date'] = df['transit_timestamp'].dt.date\n",
    "df['hour'] = df['transit_timestamp'].dt.hour\n",
    "df.drop([\"transit_timestamp\"], axis = 1)\n",
    "\n",
    "# Aggregating total ridership across different payment methods.\n",
    "df = df.groupby(['station_complex_id', 'date', 'hour'], as_index = False).agg(ridership = pd.NamedAgg(column = \"ridership\", aggfunc = \"sum\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e91eb69c-1587-442f-9fa5-472f34eacbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44145 missing entries detected for stations/hours with zero ridership.\n"
     ]
    }
   ],
   "source": [
    "# If there were 0 riders for a certain station at a certain hour, that data is not recored into the dataset.\n",
    "# Therefore, we need to fill in these values with 0 riders.\n",
    "\n",
    "# First, create a df for the cartesian product of all possibles dates, hours, and station complexes.\n",
    "all_stations = df[[\"station_complex_id\"]].drop_duplicates().reset_index(drop=True)         # 424 station complexes\n",
    "all_hours = pd.DataFrame({'hour':[i for i in range(24)]})                                  # 24 hours\n",
    "all_dates = df[['date']].drop_duplicates().reset_index(drop = True)                        # Jan-Apr = 31 + 29 + 30 + 31 = 121 days\n",
    "\n",
    "dates_hours = pd.merge(all_dates, all_hours, how = \"cross\")\n",
    "dates_hours_stations = pd.merge(dates_hours, all_stations, how = \"cross\")\n",
    "# dates_hours_stations contains every possible date, hour, and station complex in the recording period.\n",
    "\n",
    "n_missing_values = dates_hours_stations.shape[0] - df.shape[0]\n",
    "\n",
    "# Left join dates_hours_stations with the subway dataset, so that we have df with every date, hour, and station complex, with NaNs\n",
    "# where there was no data recorded in the initial dataset.\n",
    "df = pd.merge(dates_hours_stations, df, on = [\"date\", \"hour\", \"station_complex_id\"], how = \"left\")\n",
    "\n",
    "# Replace NaN values with 0.\n",
    "df = df.fillna(0)\n",
    "\n",
    "print(f\"{n_missing_values} missing entries detected for stations/hours with zero ridership.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d67bfe-0ecd-4b4b-8e02-9232cb89b720",
   "metadata": {},
   "source": [
    "# Generating Datapoints\n",
    "\n",
    "The data will be split as follows:\n",
    "\n",
    "- Training Data : 01/01/2025 - 03/31/25\n",
    "\n",
    "- Validation Data : 04/01/2025 - 04/16/2025\n",
    "\n",
    "- Testing Data : 04/17/2025 - 04/30/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b4aec0a-cdbb-4669-9cbf-74133005b622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on points in the range [01/01/2025, 04/01/2025)\n",
    "train_df = df[df[\"date\"] < datetime.date(2025, 4, 1)]\n",
    "\n",
    "# Validating on points in the range [04/01/2025, 04/17/2025)\n",
    "val_df   = df[(df[\"date\"] >= datetime.date(2025, 4, 1)) & (df[\"date\"] < datetime.date(2025, 4, 17))]\n",
    "\n",
    "# Testing on points in the range [04/17/2025, 05/01/2025)\n",
    "test_df  = df[df[\"date\"] >= datetime.date(2025, 4, 17)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "248ae59e-9e39-4592-b6e4-7bd944d266d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_from_df(inp_df, lookback = 24, prediction_horizon = 6):\n",
    "    \"\"\"\n",
    "    Function for creating (X, y) datapoints from an input dataframe, where X = (latitude, longitude, r_{t}, ..., r_{t + lookback}) and \n",
    "    y = (r_{t + lookback + 1}, ..., r_{t + lookback + prediction_horizon - 1}), where r_{i} is the ridership at a given hour/day for \n",
    "    the station at the specified latitude and longitude.\n",
    "\n",
    "    Inputs:\n",
    "        inp_df: Pandas Dataframe object with columns [\"station_complex_id\", \"date\", \"hour\", \"ridership\"] where there are NO gaps in the\n",
    "        date/hour datapoints.\n",
    "        \n",
    "        lookback: Size of lookback window in hours, default: 24.\n",
    "        \n",
    "        prediction_horizon: Size of prediction horizon in hours, default: 6.\n",
    "\n",
    "    Outputs:\n",
    "        X: Input datapoints\n",
    "        y: \n",
    "    \"\"\"\n",
    "\n",
    "    def generate_data_points_by_station_complex(x):\n",
    "        \"\"\"\n",
    "        Helper function for generating time series windows using numpy.lib.stride_tricks.sliding_window_view(), and adding them to\n",
    "        the total data_points array.\n",
    "        \"\"\"\n",
    "\n",
    "        # Make sure data is ordered so that it is a contnuous count of the ridership, i.e. each subsequent row is the following hour's\n",
    "        # ridership., for every hour in the timeframe of inp_df, for this specific station.\n",
    "        \n",
    "        x = x.sort_values(by = [\"date\", \"hour\"])\n",
    "        x.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Get latitude of this specific station, construct array of latitudes to concatenate in front of the time series window.\n",
    "        lat = station_info_df[station_info_df[\"station_complex_id\"] == x[\"station_complex_id\"][0]][\"latitude\"].iloc[0]\n",
    "        lats = np.full((x.shape[0] - lookback - prediction_horizon + 1, 1), lat)\n",
    "\n",
    "        # Get longitude of this specific station, construct array of latitudes to concatenate in front of the time series window.\n",
    "        long = station_info_df[station_info_df[\"station_complex_id\"] == x[\"station_complex_id\"][0]][\"longitude\"].iloc[0]\n",
    "        longs = np.full((x.shape[0] - lookback - prediction_horizon + 1, 1), long)\n",
    "\n",
    "        # Need numpy array for sliding_window_view()\n",
    "        riderships_array = x[\"ridership\"].to_numpy()\n",
    "        \n",
    "        # https://numpy.org/doc/stable/reference/generated/numpy.lib.stride_tricks.sliding_window_view.html\n",
    "        window = sliding_window_view(riderships_array, lookback + prediction_horizon)\n",
    "\n",
    "        # Add longitudes.\n",
    "        window_longs = np.concatenate((longs, window), axis = 1)\n",
    "\n",
    "        # Add latitudes.\n",
    "        window_lat_longs = np.concatenate((lats, window_longs), axis = 1)\n",
    "        \n",
    "        # This function iteratively concatenates new data_points to the existing data_points array, constructed in the main function.\n",
    "        nonlocal data_points\n",
    "        data_points = np.concatenate((data_points, window_lat_longs), axis = 0)\n",
    "\n",
    "    # Constructing empty array to store generated datapoints. Factor of 2 in size to account for latitude and longitude.\n",
    "    data_points = np.empty((0, 2 + lookback + prediction_horizon))\n",
    "    \n",
    "    # Using groupby to generate separate data points for each station_complex\n",
    "    inp_df.groupby([\"station_complex_id\"], as_index = False).apply(generate_data_points_by_station_complex)\n",
    "\n",
    "    # First two points are latitude, longitude, next n = lookback points are the lookback window.\n",
    "    X = data_points[:, :2 + lookback]\n",
    "\n",
    "    # Next m = prediction_horizon points are the points to be predicted.\n",
    "    y = data_points[:, 2 + lookback:]\n",
    "\n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf2c162b-caa7-4550-b9e6-3111d34caa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x5/mt3y0q2n65zcpd0hz503pkx00000gn/T/ipykernel_92060/2489767023.py:60: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  inp_df.groupby([\"station_complex_id\"], as_index = False).apply(generate_data_points_by_station_complex)\n",
      "/var/folders/x5/mt3y0q2n65zcpd0hz503pkx00000gn/T/ipykernel_92060/2489767023.py:60: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  inp_df.groupby([\"station_complex_id\"], as_index = False).apply(generate_data_points_by_station_complex)\n",
      "/var/folders/x5/mt3y0q2n65zcpd0hz503pkx00000gn/T/ipykernel_92060/2489767023.py:60: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  inp_df.groupby([\"station_complex_id\"], as_index = False).apply(generate_data_points_by_station_complex)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_X_y_from_df(train_df, 24, 6)\n",
    "X_test, y_test   = get_X_y_from_df(test_df, 24, 6)\n",
    "X_val, y_val     = get_X_y_from_df(val_df, 24, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fc65738-51b1-4fc9-bff6-b9db7c634e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "\n",
    "np.save(\"X_val.npy\", X_val)\n",
    "np.save(\"y_val.npy\", y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940cf35-3621-4d95-98da-d9a7613b4a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
