{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f1b9335-34a4-4839-baea-9c5c9dc4546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93680dea-c089-4a59-8fa2-dd7f7a12c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"MTA_Subway_Hourly_Ridership_Jan_Through_May_2025.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c6501-54bc-4365-8f81-3dd62046cca1",
   "metadata": {},
   "source": [
    "This dataframe includes the number of riders, per station complex, per hour, for every hour in every day for the months January-April.\n",
    "\n",
    "My goal is to see if I can create a model to forecast subway ridership. It should work like this: Given some ridership, that is the riders for every hour of a given time period, can I forecast the ridership in the near future? For a preliminary investigation, I will consider a lookback length of 24 hours, and a prediction horizon of 6 hours. \n",
    "\n",
    "Naturally, I need the ridership by hour for each 24 hour period, and the ridership of the next 6 hours, for each station, to construct my training dataset. The dataset as is needs to be transformed\n",
    "\n",
    "Exogenous features to consider alongside time series:\n",
    "\n",
    "- It may be important to account for the day of the week. For example, it will be more difficult to predict Saturday from Friday, than say Tuesday from Monday, due to the distinct nature of weekend and weekday ridership. \n",
    "\n",
    "- Furthermore, station information may need to be taken into consideration. Different stations experience different daily ridership patterns. Popular stations such as Times Square-42nd St. have a lot of ridership constantly, while some stations on the outer edges of the system may have little to no ridership some days.\n",
    "\n",
    "    - Note! One way to go about this is to include the (normazlied) lat/long coordinates with datapoints. This is better than simply one-hot encoding by station_complex because lat/long coordinates will contain information of where stations are with respect to one another. In a previous project, I identified a strong correlation between daily ridership pattern (on weekends) and location.\n",
    "      \n",
    "    - Note! In this dataset, there are sometimes multiple lat/long pairs listed for a given station_complex. This phenomenon is demonstrated in lat_long_discrepancy/lat_long_discrepancy.ipynb . Because I believe this is an error, I will take the most-appearing (mode) lat/long per station_complex as that station's true lat/long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "76a45ba4-c3ba-4b4b-b979-7038df650453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a df which contains station_complex, station_complex_id, latitude, longitude\n",
    "# so I can keep just station_complex_id for now, and later merge station_info_df with on=station_complex_id to \n",
    "# recover latitudes and longitudes.\n",
    "\n",
    "# As mentioned above, I take the mode(latitude) and mode(longitude) per station_complex.\n",
    "\n",
    "station_info_df = df[df[\"transit_mode\"] == \"subway\"][['station_complex_id', 'latitude', 'longitude']].drop_duplicates().reset_index(drop= True)\n",
    "station_info_df = station_info_df.groupby([\"station_complex_id\"], as_index = False).agg({\"latitude\": pd.Series.mode, \"longitude\": pd.Series.mode})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f16da1cf-d480-4182-83d9-73d92a82c49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming df to a workable form\n",
    "\n",
    "# Only considering Subway!\n",
    "df = df[df[\"transit_mode\"] == \"subway\"]\n",
    "\n",
    "# Dropping Other Unneccesary Rows!\n",
    "df = df.drop([\"transfers\", \"Georeference\", \"station_complex\", \"latitude\", \"longitude\", \"borough\"], axis = 1)\n",
    "\n",
    "# Pandas datetime format.\n",
    "df['transit_timestamp'] = pd.to_datetime(df['transit_timestamp'], format = \"%m/%d/%Y %I:%M:%S %p\")\n",
    "\n",
    "# Keeping only date and hour, then discarding transit_timestamp\n",
    "df['date'] = df['transit_timestamp'].dt.date\n",
    "df['hour'] = df['transit_timestamp'].dt.hour\n",
    "df.drop([\"transit_timestamp\"], axis = 1)\n",
    "\n",
    "# Aggregating total ridership across different payment methods.\n",
    "df = df.groupby(['station_complex_id', 'date', 'hour'], as_index = False).agg(ridership = pd.NamedAgg(column = \"ridership\", aggfunc = \"sum\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e91eb69c-1587-442f-9fa5-472f34eacbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44145 missing entries detected for stations/hours with zero ridership.\n"
     ]
    }
   ],
   "source": [
    "# If there were 0 riders for a certain station at a certain hour, that data is not recored into the dataset.\n",
    "# Therefore, we need to fill in these values with 0 riders.\n",
    "\n",
    "# First, create a df for the cartesian product of all possibles dates, hours, and station complexes.\n",
    "all_stations = df[[\"station_complex_id\"]].drop_duplicates().reset_index(drop=True)                   # 424 station complexes\n",
    "all_hours = pd.DataFrame({'hour':[i for i in range(24)]})                                            # 24 hours\n",
    "all_dates = df[['date']].drop_duplicates().reset_index(drop = True)                                  # Jan-Apr\n",
    "\n",
    "dates_hours = pd.merge(all_dates, all_hours, how = \"cross\")\n",
    "dates_hours_stations = pd.merge(dates_hours, all_stations, how = \"cross\")\n",
    "# dates_hours_stations contains every possible date, hour, and station complex in the recording period.\n",
    "\n",
    "n_missing_values = dates_hours_stations.shape[0] - df.shape[0]\n",
    "\n",
    "# Left join dates_hours_stations with the subway dataset, so that we have df with every date, hour, and station complex, with NaNs\n",
    "# where there was no data recorded in the initial dataset.\n",
    "df = pd.merge(dates_hours_stations, df, on = [\"date\", \"hour\", \"station_complex_id\"], how = \"left\")\n",
    "\n",
    "# Replace NaN values with 0.\n",
    "df = df.fillna(0)\n",
    "\n",
    "print(f\"{n_missing_values} missing entries detected for stations/hours with zero ridership.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d67bfe-0ecd-4b4b-8e02-9232cb89b720",
   "metadata": {},
   "source": [
    "# Generating Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "248ae59e-9e39-4592-b6e4-7bd944d266d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x5/mt3y0q2n65zcpd0hz503pkx00000gn/T/ipykernel_23964/520010196.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.groupby([\"station_complex\", \"borough\"]).apply(generate_data_points_by_station_complex)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_points = np.empty((0, 30))\n",
    "\n",
    "def generate_data_points_by_station_complex(x):\n",
    "    global data_points\n",
    "    \n",
    "    x = x.sort_values(by = [\"date\", \"hour\"])\n",
    "\n",
    "    riderships_array = x[\"ridership\"].to_numpy()\n",
    "    \n",
    "    data_points = np.concatenate((data_points, sliding_window_view(riderships_array, 30)), axis = 0)\n",
    "\n",
    "\n",
    "df.groupby([\"station_complex\", \"borough\"]).apply(generate_data_points_by_station_complex)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c162b-caa7-4550-b9e6-3111d34caa27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc65738-51b1-4fc9-bff6-b9db7c634e05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
